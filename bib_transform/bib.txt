
@misc{madaan_self-refine_2023,
	title = {Self-{Refine}: {Iterative} {Refinement} with {Self}-{Feedback}},
	shorttitle = {Self-{Refine}},
	url = {http://arxiv.org/abs/2303.17651},
	doi = {10.48550/arXiv.2303.17651},
	abstract = {Like people, LLMs do not always generate the best text for a given generation problem on their first try (e.g., summaries, answers, explanations). Just as people then refine their text, we introduce SELF-REFINE, a framework for similarly improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an output using an LLM, then allow the same model to provide multi-aspect feedback for its own output; finally, the same model refines its previously generated output given its own feedback. Unlike earlier work, our iterative refinement framework does not require supervised training data or reinforcement learning, and works with a single LLM. We experiment with 7 diverse tasks, ranging from review rewriting to math reasoning, demonstrating that our approach outperforms direct generation. In all tasks, outputs generated with SELF-REFINE are preferred by humans and by automated metrics over those generated directly with GPT-3.5 and GPT-4, improving on average by absolute 20\% across tasks.},
	urldate = {2023-04-04},
	publisher = {arXiv},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Welleck, Sean and Majumder, Bodhisattwa Prasad and Gupta, Shashank and Yazdanbakhsh, Amir and Clark, Peter},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17651 [cs]},
	keywords = {*, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, dview},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\2CEJU7ZM\\Madaan 等 - 2023 - Self-Refine Iterative Refinement with Self-Feedba.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\27PK2UCA\\2303.html:text/html},
}

@misc{jojic_gpt_2023,
	title = {{GPT} is becoming a {Turing} machine: {Here} are some ways to program it},
	shorttitle = {{GPT} is becoming a {Turing} machine},
	url = {http://arxiv.org/abs/2303.14310},
	doi = {10.48550/arXiv.2303.14310},
	abstract = {We demonstrate that, through appropriate prompting, GPT-3 family of models can be triggered to perform iterative behaviours necessary to execute (rather than just write or recall) programs that involve loops, including several popular algorithms found in computer science curricula or software developer interviews. We trigger execution and description of Iterations by Regimenting Self-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong repetitive structure in an example of an execution path of a target program for one particular input, 2) Prompting with fragments of execution paths, and 3) Explicitly forbidding (skipping) self-attention to parts of the generated text. On a dynamic program execution, IRSA leads to larger accuracy gains than replacing the model with the much more powerful GPT-4. IRSA has promising applications in education, as the prompts and responses resemble student assignments in data structures and algorithms classes. Our findings hold implications for evaluating LLMs, which typically target the in-context learning: We show that prompts that may not even cover one full task example can trigger algorithmic behaviour, allowing solving problems previously thought of as hard for LLMs, such as logical puzzles. Consequently, prompt design plays an even more critical role in LLM performance than previously recognized.},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {Jojic, Ana and Wang, Zhen and Jojic, Nebojsa},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14310 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\F2SQYSA8\\Jojic 等 - 2023 - GPT is becoming a Turing machine Here are some wa.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\7WUT3LTN\\2303.html:text/html},
}

@misc{liu_gpteval_2023,
	title = {{GPTEval}: {NLG} {Evaluation} using {GPT}-4 with {Better} {Human} {Alignment}},
	shorttitle = {{GPTEval}},
	url = {http://arxiv.org/abs/2303.16634},
	doi = {10.48550/arXiv.2303.16634},
	abstract = {The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present GPTEval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that GPTEval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts.},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16634 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\N3Q228AT\\Liu 等 - 2023 - GPTEval NLG Evaluation using GPT-4 with Better Hu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\P3G58FAH\\2303.html:text/html},
}

@misc{bian_chatgpt_2023,
	title = {{ChatGPT} is a {Knowledgeable} but {Inexperienced} {Solver}: {An} {Investigation} of {Commonsense} {Problem} in {Large} {Language} {Models}},
	shorttitle = {{ChatGPT} is a {Knowledgeable} but {Inexperienced} {Solver}},
	url = {http://arxiv.org/abs/2303.16421},
	doi = {10.48550/arXiv.2303.16421},
	abstract = {Large language models (LLMs) such as ChatGPT and GPT-4 have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point for LLMs. It remains unclear that: (1) Can GPTs effectively answer commonsense questions? (2) Are GPTs knowledgeable in commonsense? (3) Are GPTs aware of the underlying commonsense knowledge for answering a specific question? (4) Can GPTs effectively leverage commonsense for answering questions? To evaluate the above commonsense problems, we conduct a series of experiments to evaluate ChatGPT's commonsense abilities, and the experimental results show that: (1) GPTs can achieve good QA accuracy in commonsense tasks, while they still struggle with certain types of knowledge. (2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense knowledge for answering a specific question, i.e., ChatGPT does not precisely know what commonsense knowledge is required to answer a question. The above findings raise the need to investigate better mechanisms for utilizing commonsense knowledge in LLMs, such as instruction following, better commonsense guidance, etc.},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {Bian, Ning and Han, Xianpei and Sun, Le and Lin, Hongyu and Lu, Yaojie and He, Ben},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16421 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\CBZMZA55\\Bian 等 - 2023 - ChatGPT is a Knowledgeable but Inexperienced Solve.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\FBVQKZFN\\2303.html:text/html},
}

@misc{scheurer_training_2023,
	title = {Training {Language} {Models} with {Language} {Feedback} at {Scale}},
	url = {http://arxiv.org/abs/2303.16755},
	doi = {10.48550/arXiv.2303.16755},
	abstract = {Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate ILF's effectiveness on a carefully-controlled toy task and a realistic summarization task. Our experiments demonstrate that large language models accurately incorporate feedback and that finetuning with ILF scales well with the dataset size, even outperforming finetuning on human summaries. Learning from both language and comparison feedback outperforms learning from each alone, achieving human-level summarization performance.},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {Scheurer, Jérémy and Campos, Jon Ander and Korbak, Tomasz and Chan, Jun Shern and Chen, Angelica and Cho, Kyunghyun and Perez, Ethan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16755 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\2YLR2LM8\\Scheurer 等 - 2023 - Training Language Models with Language Feedback at.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\ZKLEYAME\\2303.html:text/html},
}

@misc{he_annollm_2023,
	title = {{AnnoLLM}: {Making} {Large} {Language} {Models} to {Be} {Better} {Crowdsourced} {Annotators}},
	shorttitle = {{AnnoLLM}},
	url = {http://arxiv.org/abs/2303.16854},
	doi = {10.48550/arXiv.2303.16854},
	abstract = {Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data. We conduct experiments on three tasks, including user input and keyword relevance assessment, BoolQ and WiC. The annotation results from GPT-3.5 surpasses those from crowdsourced annotation for user input and keyword relevance assessment. Additionally, for the other two tasks, GPT-3.5 achieves results that are comparable to those obtained through crowdsourced annotation.},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, A.-Long and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16854 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\IZDA5NAH\\He 等 - 2023 - AnnoLLM Making Large Language Models to Be Better.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\A4NW8NLB\\2303.html:text/html},
}

@misc{jourdan_text_2023,
	title = {Text revision in {Scientific} {Writing} {Assistance}: {An} {Overview}},
	shorttitle = {Text revision in {Scientific} {Writing} {Assistance}},
	url = {http://arxiv.org/abs/2303.16726},
	doi = {10.48550/arXiv.2303.16726},
	abstract = {Writing a scientific article is a challenging task as it is a highly codified genre. Good writing skills are essential to properly convey ideas and results of research work. Since the majority of scientific articles are currently written in English, this exercise is all the more difficult for non-native English speakers as they additionally have to face language issues. This article aims to provide an overview of text revision in writing assistance in the scientific domain. We will examine the specificities of scientific writing, including the format and conventions commonly used in research articles. Additionally, this overview will explore the various types of writing assistance tools available for text revision. Despite the evolution of the technology behind these tools through the years, from rule-based approaches to deep neural-based ones, challenges still exist (tools' accessibility, limited consideration of the context, inexplicit use of discursive information, etc.)},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {Jourdan, Léane and Boudin, Florian and Dufour, Richard and Hernandez, Nicolas},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16726 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\AEK4DRZ5\\Jourdan 等 - 2023 - Text revision in Scientific Writing Assistance An.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\YJKX3LDX\\2303.html:text/html},
}

@misc{chen_lmexplainer_2023,
	title = {{LMExplainer}: a {Knowledge}-{Enhanced} {Explainer} for {Language} {Models}},
	shorttitle = {{LMExplainer}},
	url = {http://arxiv.org/abs/2303.16537},
	doi = {10.48550/arXiv.2303.16537},
	abstract = {Large language models (LMs) such as GPT-4 are very powerful and can process different kinds of natural language processing (NLP) tasks. However, it can be difficult to interpret the results due to the multi-layer nonlinear model structure and millions of parameters. Lack of understanding of how the model works can make the model unreliable and dangerous for everyday users in real-world scenarios. Most recent works exploit the weights of attention to provide explanations for model predictions. However, pure attention-based explanation is unable to support the growing complexity of the models, and cannot reason about their decision-making processes. Thus, we propose LMExplainer, a knowledge-enhanced interpretation module for language models that can provide human-understandable explanations. We use a knowledge graph (KG) and a graph attention neural network to extract the key decision signals of the LM. We further explore whether interpretation can also help AI understand the task better. Our experimental results show that LMExplainer outperforms existing LM+KG methods on CommonsenseQA and OpenBookQA. We also compare the explanation results with generated explanation methods and human-annotated results. The comparison shows our method can provide more comprehensive and clearer explanations. LMExplainer demonstrates the potential to enhance model performance and furnish explanations for the reasoning processes of models in natural language.},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {Chen, Zichen and Singh, Ambuj K. and Sra, Misha},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16537 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\5XWX89YH\\Chen 等 - 2023 - LMExplainer a Knowledge-Enhanced Explainer for La.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\ETCF96JN\\2303.html:text/html},
}

@misc{orhan_recognition_2023,
	title = {Recognition, recall, and retention of few-shot memories in large language models},
	url = {http://arxiv.org/abs/2303.17557},
	doi = {10.48550/arXiv.2303.17557},
	abstract = {The training of modern large language models (LLMs) takes place in a regime where most training examples are seen only a few times by the model during the course of training. What does a model remember about such examples seen only a few times during training and how long does that memory persist in the face of continuous training with new examples? Here, we investigate these questions through simple recognition, recall, and retention experiments with LLMs. In recognition experiments, we ask if the model can distinguish the seen example from a novel example; in recall experiments, we ask if the model can correctly recall the seen example when cued by a part of it; and in retention experiments, we periodically probe the model's memory for the original examples as the model is trained continuously with new examples. We find that a single exposure is generally sufficient for a model to achieve near perfect accuracy even in very challenging recognition experiments. We estimate that the recognition performance of even small language models easily exceeds human recognition performance reported in similar experiments with humans (Shepard, 1967). Achieving near perfect recall takes more exposures, but most models can do it in just 3 exposures. The flip side of this remarkable capacity for fast learning is that precise memories are quickly overwritten: recall performance for the original examples drops steeply over the first 10 training updates with new examples, followed by a more gradual decline. Even after 100K updates, however, some of the original examples are still recalled near perfectly. A qualitatively similar retention pattern has been observed in human long-term memory retention studies before (Bahrick, 1984). Finally, recognition is much more robust to interference than recall and memory for natural language sentences is generally superior to memory for stimuli without structure.},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {Orhan, A. Emin},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17557 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\BD2EM7GI\\Orhan - 2023 - Recognition, recall, and retention of few-shot mem.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\RY87NMFU\\2303.html:text/html},
}

@misc{west_advances_2023,
	title = {Advances in apparent conceptual physics reasoning in {ChatGPT}-4},
	url = {http://arxiv.org/abs/2303.17012},
	doi = {10.48550/arXiv.2303.17012},
	abstract = {ChatGPT is built on a large language model trained on an enormous corpus of human text to emulate human conversation. Despite lacking any explicit programming regarding the laws of physics, recent work by Kortemeyer (2023) has demonstrated that ChatGPT-3.5 could pass an introductory physics course at some nominal level and register something close to a minimal understanding of Newtonian Mechanics on the Force Concept Inventory. This work replicates those results and also demonstrates that the latest version, ChatGPT-4, has reached a much higher mark in the latter context. Indeed, its responses come quite close to perfectly demonstrating expert-level competence, with a few very notable exceptions and limitations. We briefly comment on the implications of this for the future of physics education and pedagogy.},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {West, Colin G.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17012 [physics]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Physics - Physics Education},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\UENBDCYW\\West - 2023 - Advances in apparent conceptual physics reasoning .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\CJ8CG99N\\2303.html:text/html},
}

@misc{kim_language_2023,
	title = {Language {Models} can {Solve} {Computer} {Tasks}},
	url = {http://arxiv.org/abs/2303.17491},
	doi = {10.48550/arXiv.2303.17491},
	abstract = {Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL method, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting. We find that RCI combined with CoT performs better than either separately.},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {Kim, Geunwoo and Baldi, Pierre and McAleer, Stephen},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17491 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\7RMBXMM3\\Kim 等 - 2023 - Language Models can Solve Computer Tasks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\3ZKAJXAB\\2303.html:text/html},
}

@misc{wu_bloomberggpt_2023,
	title = {{BloombergGPT}: {A} {Large} {Language} {Model} for {Finance}},
	shorttitle = {{BloombergGPT}},
	url = {http://arxiv.org/abs/2303.17564},
	doi = {10.48550/arXiv.2303.17564},
	abstract = {The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. As a next step, we plan to release training logs (Chronicles) detailing our experience in training BloombergGPT.},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17564 [cs, q-fin]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Quantitative Finance - General Finance},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\HT4C8DEJ\\Wu 等 - 2023 - BloombergGPT A Large Language Model for Finance.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\Y7S6VZZH\\2303.html:text/html},
}

@misc{shen_hugginggpt_2023,
	title = {{HuggingGPT}: {Solving} {AI} {Tasks} with {ChatGPT} and its {Friends} in {HuggingFace}},
	shorttitle = {{HuggingGPT}},
	url = {http://arxiv.org/abs/2303.17580},
	doi = {10.48550/arXiv.2303.17580},
	abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in HuggingFace, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards AGI.},
	urldate = {2023-04-01},
	publisher = {arXiv},
	author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17580 [cs]
version: 1},
	keywords = {*, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\HW5AM5PT\\Shen 等 - 2023 - HuggingGPT Solving AI Tasks with ChatGPT and its .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\3AFFVF64\\2303.html:text/html},
}

@misc{liang_taskmatrixai_2023,
	title = {{TaskMatrix}.{AI}: {Completing} {Tasks} by {Connecting} {Foundation} {Models} with {Millions} of {APIs}},
	shorttitle = {{TaskMatrix}.{AI}},
	url = {http://arxiv.org/abs/2303.16434},
	abstract = {Artificial Intelligence (AI) has made incredible progress recently. On the one hand, advanced foundation models like ChatGPT can offer powerful conversation, in-context learning and code generation abilities on a broad range of open-domain tasks. They can also generate high-level solution outlines for domain-specific tasks based on the common sense knowledge they have acquired. However, they still face difficulties with some specialized tasks because they lack enough domain-specific data during pre-training or they often have errors in their neural network computations on those tasks that need accurate executions. On the other hand, there are also many existing models and systems (symbolic-based or neural-based) that can do some domain-specific tasks very well. However, due to the different implementation or working mechanisms, they are not easily accessible or compatible with foundation models. Therefore, there is a clear and pressing need for a mechanism that can leverage foundation models to propose task solution outlines and then automatically match some of the sub-tasks in the outlines to the off-the-shelf models and systems with special functionalities to complete them. Inspired by this, we introduce TaskMatrix.AI as a new AI ecosystem that connects foundation models with millions of APIs for task completion. Unlike most previous work that aimed to improve a single AI model, TaskMatrix.AI focuses more on using existing foundation models (as a brain-like central system) and APIs of other AI models and systems (as sub-task solvers) to achieve diversified tasks in both digital and physical domains. As a position paper, we will present our vision of how to build such an ecosystem, explain each key component, and use study cases to illustrate both the feasibility of this vision and the main challenges we need to address next.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Liang, Yaobo and Wu, Chenfei and Song, Ting and Wu, Wenshan and Xia, Yan and Liu, Yu and Ou, Yang and Lu, Shuai and Ji, Lei and Mao, Shaoguang and Wang, Yun and Shou, Linjun and Gong, Ming and Duan, Nan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16434 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, dview},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\C5X2ST5L\\Liang 等 - 2023 - TaskMatrix.AI Completing Tasks by Connecting Foun.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\PHU9KNQL\\2303.html:text/html},
}

@misc{madaan_text_2022,
	title = {Text and {Patterns}: {For} {Effective} {Chain} of {Thought}, {It} {Takes} {Two} to {Tango}},
	shorttitle = {Text and {Patterns}},
	url = {http://arxiv.org/abs/2209.07686},
	doi = {10.48550/arXiv.2209.07686},
	abstract = {The past decade has witnessed dramatic gains in natural language processing and an unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (CoT) prompting. Specifically, CoT pushes the performance of large language models in a few-shot setup by augmenting the prompts with intermediate steps. Despite impressive results across various tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of CoT-based few-shot prompting mechanisms in large language models. We first systematically identify and define the key components of a prompt: symbols, patterns, and text. Then, we devise and conduct an exhaustive set of experiments across four different tasks, by querying the model with counterfactual prompts where only one of these components is altered. Our experiments across three models (PaLM, GPT-3, and CODEX) reveal several surprising findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success of CoT. Second, our results conclude that the primary role of intermediate steps may not be to facilitate learning how to solve a task. The intermediate steps are rather a beacon for the model to realize what symbols to replicate in the output to form a factual answer. Further, text imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis reveals that a symbiotic relationship between text and patterns explains the success of few-shot prompting: text helps extract commonsense from the question to help patterns, and patterns enforce task understanding and direct text generation.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Madaan, Aman and Yazdanbakhsh, Amir},
	month = oct,
	year = {2022},
	note = {arXiv:2209.07686 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\B9H624US\\Madaan 和 Yazdanbakhsh - 2022 - Text and Patterns For Effective Chain of Thought,.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\CI2ZKB89\\2209.html:text/html},
}

@misc{tandon_learning_2022,
	title = {Learning to {Repair}: {Repairing} model output errors after deployment using a dynamic memory of feedback},
	shorttitle = {Learning to {Repair}},
	url = {http://arxiv.org/abs/2112.09737},
	doi = {10.48550/arXiv.2112.09737},
	abstract = {Large language models (LMs), while powerful, are not immune to mistakes, but can be difficult to retrain. Our goal is for an LM to continue to improve after deployment, without retraining, using feedback from the user. Our approach pairs an LM with (i) a growing memory of cases where the user identified an output error and provided general feedback on how to correct it (ii) a corrector model, trained to translate this general feedback into specific edits to repair the model output. Given a new, unseen input, our model can then use feedback from similar, past cases to repair output errors that may occur. We instantiate our approach using an existing, fixed model for script generation, that takes a goal (e.g., "bake a cake") and generates a partially ordered sequence of actions to achieve that goal, sometimes containing errors. Our memory-enhanced system, FBNet, learns to apply user feedback to repair such errors (up to 30 points improvement), while making a start at avoiding similar past mistakes on new, unseen examples (up to 7 points improvement in a controlled setting). This is a first step towards strengthening deployed models, potentially broadening their utility. Our code and data is available at https://github.com/allenai/interscript/.},
	urldate = {2023-03-30},
	publisher = {arXiv},
	author = {Tandon, Niket and Madaan, Aman and Clark, Peter and Yang, Yiming},
	month = may,
	year = {2022},
	note = {arXiv:2112.09737 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\BL7ADGNX\\Tandon 等 - 2022 - Learning to Repair Repairing model output errors .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\AB3FJYUA\\2112.html:text/html},
}

@misc{yu_nature_2023,
	title = {Nature {Language} {Reasoning}, {A} {Survey}},
	url = {http://arxiv.org/abs/2303.14725},
	doi = {10.48550/arXiv.2303.14725},
	abstract = {This survey paper proposes a clearer view of natural language reasoning in the field of Natural Language Processing (NLP), both conceptually and practically. Conceptually, we provide a distinct definition for natural language reasoning in NLP, based on both philosophy and NLP scenarios, discuss what types of tasks require reasoning, and introduce a taxonomy of reasoning. Practically, we conduct a comprehensive literature review on natural language reasoning in NLP, mainly covering classical logical reasoning, natural language inference, multi-hop question answering, and commonsense reasoning. The paper also identifies and views backward reasoning, a powerful paradigm for multi-step reasoning, and introduces defeasible reasoning as one of the most important future directions in natural language reasoning research. We focus on single-modality unstructured natural language text, excluding neuro-symbolic techniques and mathematical reasoning.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Yu, Fei and Zhang, Hongbo and Wang, Benyou},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14725 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\BRTSEUJ2\\Yu 等 - 2023 - Nature Language Reasoning, A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\CE6XCHZN\\2303.html:text/html},
}

@misc{jia_chain--thought_2023,
	title = {Chain-of-{Thought} {Predictive} {Control}},
	url = {http://arxiv.org/abs/2304.00776},
	doi = {10.48550/arXiv.2304.00776},
	abstract = {We study generalizable policy learning from demonstrations for complex low-level control tasks (e.g., contact-rich object manipulations). We propose an imitation learning method that incorporates the idea of temporal abstraction and the planning capabilities from Hierarchical RL (HRL) in a novel and effective manner. As a step towards decision foundation models, our design can utilize scalable, albeit highly sub-optimal, demonstrations. Specifically, we find certain short subsequences of the demos, i.e. the chain-of-thought (CoT), reflect their hierarchical structures by marking the completion of subgoals in the tasks. Our model learns to dynamically predict the entire CoT as coherent and structured long-term action guidance and consistently outperforms typical two-stage subgoal-conditioned policies. On the other hand, such CoT facilitates generalizable policy learning as they exemplify the decision patterns shared among demos (even those with heavy noises and randomness). Our method, Chain-of-Thought Predictive Control (CoTPC), significantly outperforms existing ones on challenging low-level manipulation tasks from scalable yet highly sub-optimal demos.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Jia, Zhiwei and Liu, Fangchen and Thumuluri, Vineet and Chen, Linghao and Huang, Zhiao and Su, Hao},
	month = apr,
	year = {2023},
	note = {arXiv:2304.00776 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\5GB2ITWT\\Jia 等 - 2023 - Chain-of-Thought Predictive Control.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\HRJB45U5\\2304.html:text/html},
}

@misc{chen_when_2023,
	title = {When do you need {Chain}-of-{Thought} {Prompting} for {ChatGPT}?},
	url = {http://arxiv.org/abs/2304.03262},
	doi = {10.48550/arXiv.2304.03262},
	abstract = {Chain-of-Thought (CoT) prompting can effectively elicit complex multi-step reasoning from Large Language Models{\textasciitilde}(LLMs). For example, by simply adding CoT instruction ``Let's think step-by-step'' to each input query of MultiArith dataset, GPT-3's accuracy can be improved from 17.7{\textbackslash}\% to 78.7{\textbackslash}\%. However, it is not clear whether CoT is still effective on more recent instruction finetuned (IFT) LLMs such as ChatGPT. Surprisingly, on ChatGPT, CoT is no longer effective for certain tasks such as arithmetic reasoning while still keeping effective on other reasoning tasks. Moreover, on the former tasks, ChatGPT usually achieves the best performance and can generate CoT even without being instructed to do so. Hence, it is plausible that ChatGPT has already been trained on these tasks with CoT and thus memorized the instruction so it implicitly follows such an instruction when applied to the same queries, even without CoT. Our analysis reflects a potential risk of overfitting/bias toward instructions introduced in IFT, which becomes more common in training LLMs. In addition, it indicates possible leakage of the pretraining recipe, e.g., one can verify whether a dataset and instruction were used in training ChatGPT. Our experiments report new baseline results of ChatGPT on a variety of reasoning tasks and shed novel insights into LLM's profiling, instruction memorization, and pretraining dataset leakage.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Chen, Jiuhai and Chen, Lichang and Huang, Heng and Zhou, Tianyi},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03262 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\NXFTXLDP\\Chen 等 - 2023 - When do you need Chain-of-Thought Prompting for Ch.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\AK8WPWA5\\2304.html:text/html},
}

@misc{francis_core_2023,
	title = {Core {Challenges} in {Embodied} {Vision}-{Language} {Planning}},
	url = {http://arxiv.org/abs/2304.02738},
	doi = {10.48550/arXiv.2304.02738},
	abstract = {Recent advances in the areas of Multimodal Machine Learning and Artificial Intelligence (AI) have led to the development of challenging tasks at the intersection of Computer Vision, Natural Language Processing, and Robotics. Whereas many approaches and previous survey pursuits have characterised one or two of these dimensions, there has not been a holistic analysis at the center of all three. Moreover, even when combinations of these topics are considered, more focus is placed on describing, e.g., current architectural methods, as opposed to also illustrating high-level challenges and opportunities for the field. In this survey paper, we discuss Embodied Vision-Language Planning (EVLP) tasks, a family of prominent embodied navigation and manipulation problems that jointly leverage computer vision and natural language for interaction in physical environments. We propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the current and new algorithmic approaches, metrics, simulators, and datasets used for EVLP tasks. Finally, we present the core challenges that we believe new EVLP works should seek to address, and we advocate for task construction that enables model generalisability and furthers real-world deployment.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Francis, Jonathan and Kitamura, Nariaki and Labelle, Felix and Lu, Xiaopeng and Navarro, Ingrid and Oh, Jean},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02738 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\5B5WBVUX\\Francis 等 - 2023 - Core Challenges in Embodied Vision-Language Planni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\V7SVRL4J\\2304.html:text/html},
}

@misc{khan_natural_2023,
	title = {Natural {Language} {Robot} {Programming}: {NLP} integrated with autonomous robotic grasping},
	shorttitle = {Natural {Language} {Robot} {Programming}},
	url = {http://arxiv.org/abs/2304.02993},
	doi = {10.48550/arXiv.2304.02993},
	abstract = {In this paper, we present a grammar-based natural language framework for robot programming, specifically for pick-and-place tasks. Our approach uses a custom dictionary of action words, designed to store together words that share meaning, allowing for easy expansion of the vocabulary by adding more action words from a lexical database. We validate our Natural Language Robot Programming (NLRP) framework through simulation and real-world experimentation, using a Franka Panda robotic arm equipped with a calibrated camera-in-hand and a microphone. Participants were asked to complete a pick-and-place task using verbal commands, which were converted into text using Google's Speech-to-Text API and processed through the NLRP framework to obtain joint space trajectories for the robot. Our results indicate that our approach has a high system usability score. The framework's dictionary can be easily extended without relying on transfer learning or large data sets. In the future, we plan to compare the presented framework with different approaches of human-assisted pick-and-place tasks via a comprehensive user study.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Khan, Muhammad Arshad and Kenney, Max and Painter, Jack and Kamale, Disha and Batista-Navarro, Riza and Ghalamzan-E, Amir},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02993 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\QM6CGBZV\\Khan 等 - 2023 - Natural Language Robot Programming NLP integrated.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\VPHZ9JQB\\2304.html:text/html},
}

@misc{peng_instruction_2023,
	title = {Instruction {Tuning} with {GPT}-4},
	url = {http://arxiv.org/abs/2304.03277},
	doi = {10.48550/arXiv.2304.03277},
	abstract = {Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03277 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, qview},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\TWKJKDDU\\Peng 等 - 2023 - Instruction Tuning with GPT-4.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\YWDCV645\\2304.html:text/html},
}

@misc{noever_multimodal_2023,
	title = {The {Multimodal} {And} {Modular} {Ai} {Chef}: {Complex} {Recipe} {Generation} {From} {Imagery}},
	shorttitle = {The {Multimodal} {And} {Modular} {Ai} {Chef}},
	url = {http://arxiv.org/abs/2304.02016},
	doi = {10.48550/arXiv.2304.02016},
	abstract = {The AI community has embraced multi-sensory or multi-modal approaches to advance this generation of AI models to resemble expected intelligent understanding. Combining language and imagery represents a familiar method for specific tasks like image captioning or generation from descriptions. This paper compares these monolithic approaches to a lightweight and specialized method based on employing image models to label objects, then serially submitting this resulting object list to a large language model (LLM). This use of multiple Application Programming Interfaces (APIs) enables better than 95\% mean average precision for correct object lists, which serve as input to the latest Open AI text generator (GPT-4). To demonstrate the API as a modular alternative, we solve the problem of a user taking a picture of ingredients available in a refrigerator, then generating novel recipe cards tailored to complex constraints on cost, preparation time, dietary restrictions, portion sizes, and multiple meal plans. The research concludes that monolithic multimodal models currently lack the coherent memory to maintain context and format for this task and that until recently, the language models like GPT-2/3 struggled to format similar problems without degenerating into repetitive or non-sensical combinations of ingredients. For the first time, an AI chef or cook seems not only possible but offers some enhanced capabilities to augment human recipe libraries in pragmatic ways. The work generates a 100-page recipe book featuring the thirty top ingredients using over 2000 refrigerator images as initializing lists.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Noever, David and Noever, Samantha Elizabeth Miller},
	month = mar,
	year = {2023},
	note = {arXiv:2304.02016 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\8QH86AV9\\Noever 和 Noever - 2023 - The Multimodal And Modular Ai Chef Complex Recipe.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\REWN57ZZ\\2304.html:text/html},
}

@misc{gao_human-like_2023,
	title = {Human-like {Summarization} {Evaluation} with {ChatGPT}},
	url = {http://arxiv.org/abs/2304.02554},
	doi = {10.48550/arXiv.2304.02554},
	abstract = {Evaluating text summarization is a challenging problem, and existing evaluation metrics are far from satisfactory. In this study, we explored ChatGPT's ability to perform human-like summarization evaluation using four human evaluation methods on five datasets. We found that ChatGPT was able to complete annotations relatively smoothly using Likert scale scoring, pairwise comparison, Pyramid, and binary factuality evaluation. Additionally, it outperformed commonly used automatic evaluation metrics on some datasets. Furthermore, we discussed the impact of different prompts, compared its performance with that of human evaluation, and analyzed the generated explanations and invalid responses.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Gao, Mingqi and Ruan, Jie and Sun, Renliang and Yin, Xunjian and Yang, Shiping and Wan, Xiaojun},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02554 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\75DM8LGJ\\Gao 等 - 2023 - Human-like Summarization Evaluation with ChatGPT.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\7IPSP373\\2304.html:text/html},
}

@misc{liu_summary_2023,
	title = {Summary of {ChatGPT}/{GPT}-4 {Research} and {Perspective} {Towards} the {Future} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2304.01852},
	doi = {10.48550/arXiv.2304.01852},
	abstract = {This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT/GPT-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and Wu, Zihao and Zhu, Dajiang and Li, Xiang and Qiang, Ning and Shen, Dingang and Liu, Tianming and Ge, Bao},
	month = apr,
	year = {2023},
	note = {arXiv:2304.01852 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\8WBJI856\\Liu 等 - 2023 - Summary of ChatGPTGPT-4 Research and Perspective .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\ZN9I5PKZ\\2304.html:text/html},
}

@misc{hu_llm-adapters_2023,
	title = {{LLM}-{Adapters}: {An} {Adapter} {Family} for {Parameter}-{Efficient} {Fine}-{Tuning} of {Large} {Language} {Models}},
	shorttitle = {{LLM}-{Adapters}},
	url = {http://arxiv.org/abs/2304.01933},
	doi = {10.48550/arXiv.2304.01933},
	abstract = {The success of large language models (LLMs), like GPT-3 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by fine-tuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, OPT, and GPT-J, as well as widely used adapters such as Series adapter, Parallel adapter, and LoRA. The framework is designed to be research-friendly, efficient, modular, and extendable, allowing the integration of new adapters and the evaluation of them with new and larger-scale LLMs. Furthermore, to evaluate the effectiveness of adapters in LLMs-Adapters, we conduct experiments on six math reasoning datasets. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to that of powerful LLMs (175B) in zero-shot inference on simple math reasoning datasets. Overall, we provide a promising framework for fine-tuning large LLMs on downstream tasks. We believe the proposed LLMs-Adapters will advance adapter-based PEFT research, facilitate the deployment of research pipelines, and enable practical applications to real-world systems.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Hu, Zhiqiang and Lan, Yihuai and Wang, Lei and Xu, Wanyu and Lim, Ee-Peng and Lee, Roy Ka-Wei and Bing, Lidong and Poria, Soujanya},
	month = apr,
	year = {2023},
	note = {arXiv:2304.01933 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\832PABRC\\Hu 等 - 2023 - LLM-Adapters An Adapter Family for Parameter-Effi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\96TZB2AI\\2304.html:text/html},
}

@misc{xiong_doctorglm_2023,
	title = {{DoctorGLM}: {Fine}-tuning your {Chinese} {Doctor} is not a {Herculean} {Task}},
	shorttitle = {{DoctorGLM}},
	url = {http://arxiv.org/abs/2304.01097},
	doi = {10.48550/arXiv.2304.01097},
	abstract = {The recent progress of large language models (LLMs), including ChatGPT and GPT-4, in comprehending and responding to human instructions has been remarkable. Nevertheless, these models typically perform better in English and have not been explicitly trained for the medical domain, resulting in suboptimal precision in diagnoses, drug recommendations, and other medical advice. Additionally, training and deploying a dialogue model is still believed to be impossible for hospitals, hindering the promotion of LLMs. To tackle these challenges, we have collected databases of medical dialogues in Chinese with ChatGPT's help and adopted several techniques to train an easy-deploy LLM. Remarkably, we were able to fine-tune the ChatGLM-6B on a single A100 80G in 13 hours, which means having a healthcare-purpose LLM can be very affordable. DoctorGLM is currently an early-stage engineering attempt and contain various mistakes. We are sharing it with the broader community to invite feedback and suggestions to improve its healthcare-focused capabilities: https://github.com/xionghonglin/DoctorGLM.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Xiong, Honglin and Wang, Sheng and Zhu, Yitao and Zhao, Zihao and Liu, Yuxiao and Wang, Qian and Shen, Dinggang},
	month = apr,
	year = {2023},
	note = {arXiv:2304.01097 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\IYXYUPVV\\Xiong 等 - 2023 - DoctorGLM Fine-tuning your Chinese Doctor is not .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\Q9TRQCEH\\2304.html:text/html},
}

@misc{lin_towards_2023,
	title = {Towards {Healthy} {AI}: {Large} {Language} {Models} {Need} {Therapists} {Too}},
	shorttitle = {Towards {Healthy} {AI}},
	url = {http://arxiv.org/abs/2304.00416},
	doi = {10.48550/arXiv.2304.00416},
	abstract = {Recent advances in large language models (LLMs) have led to the development of powerful AI chatbots capable of engaging in natural and human-like conversations. However, these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to be safe, trustworthy and ethical. To create healthy AI systems, we present the SafeguardGPT framework that uses psychotherapy to correct for these harmful behaviors in AI chatbots. The framework involves four types of AI agents: a Chatbot, a "User," a "Therapist," and a "Critic." We demonstrate the effectiveness of SafeguardGPT through a working example of simulating a social conversation. Our results show that the framework can improve the quality of conversations between AI chatbots and humans. Although there are still several challenges and directions to be addressed in the future, SafeguardGPT provides a promising approach to improving the alignment between AI chatbots and human values. By incorporating psychotherapy and reinforcement learning techniques, the framework enables AI chatbots to learn and adapt to human preferences and values in a safe and ethical way, contributing to the development of a more human-centric and responsible AI.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Lin, Baihan and Bouneffouf, Djallel and Cecchi, Guillermo and Varshney, Kush R.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.00416 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\UZVNP3XA\\Lin 等 - 2023 - Towards Healthy AI Large Language Models Need The.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\6TBKMMDE\\2304.html:text/html},
}

@misc{puchert_llmmaps_2023,
	title = {{LLMMaps} -- {A} {Visual} {Metaphor} for {Stratified} {Evaluation} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2304.00457},
	abstract = {Large Language Models (LLMs) have revolutionized natural language processing and demonstrated impressive capabilities in various tasks. Unfortunately, they are prone to hallucinations, where the model exposes incorrect or false information in its responses, which renders diligent evaluation approaches mandatory. While LLM performance in specific knowledge fields is often evaluated based on question and answer (Q\&A) datasets, such evaluations usually report only a single accuracy number for the entire field, a procedure which is problematic with respect to transparency and model improvement. A stratified evaluation could instead reveal subfields, where hallucinations are more likely to occur and thus help to better assess LLMs' risks and guide their further development. To support such stratified evaluations, we propose LLMMaps as a novel visualization technique that enables users to evaluate LLMs' performance with respect to Q\&A datasets. LLMMaps provide detailed insights into LLMs' knowledge capabilities in different subfields, by transforming Q\&A datasets as well as LLM responses into our internal knowledge structure. An extension for comparative visualization furthermore, allows for the detailed comparison of multiple LLMs. To assess LLMMaps we use them to conduct a comparative analysis of several state-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT and LLaMa-13B, as well as two qualitative user evaluations. All necessary source code and data for generating LLMMaps to be used in scientific publications and elsewhere will be available on GitHub.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Puchert, Patrik and Poonam, Poonam and van Onzenoodt, Christian and Ropinski, Timo},
	month = apr,
	year = {2023},
	note = {arXiv:2304.00457 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Graphics, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\GYQJTL46\\Puchert 等 - 2023 - LLMMaps -- A Visual Metaphor for Stratified Evalua.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\UIQYJMMQ\\2304.html:text/html},
}

@misc{xu_baize_2023,
	title = {Baize: {An} {Open}-{Source} {Chat} {Model} with {Parameter}-{Efficient} {Tuning} on {Self}-{Chat} {Data}},
	shorttitle = {Baize},
	url = {http://arxiv.org/abs/2304.01196},
	doi = {10.48550/arXiv.2304.01196},
	abstract = {Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize. An online demo is also available at https://huggingface.co/spaces/project-baize/baize-lora-7B.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
	month = apr,
	year = {2023},
	note = {arXiv:2304.01196 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\BG9DGGHF\\Xu 等 - 2023 - Baize An Open-Source Chat Model with Parameter-Ef.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\2NNCVXHF\\2304.html:text/html},
}

@misc{zhao_survey_2023,
	title = {A {Survey} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.18223},
	doi = {10.48550/arXiv.2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	month = mar,
	year = {2023},
	note = {arXiv:2303.18223 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\RUJHHG6E\\Zhao 等 - 2023 - A Survey of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\ZMX22YWV\\2303.html:text/html},
}

@misc{nair_dera_2023,
	title = {{DERA}: {Enhancing} {Large} {Language} {Model} {Completions} with {Dialog}-{Enabled} {Resolving} {Agents}},
	shorttitle = {{DERA}},
	url = {http://arxiv.org/abs/2303.17071},
	doi = {10.48550/arXiv.2303.17071},
	abstract = {Large language models (LLMs) have emerged as valuable tools for many natural language understanding tasks. In safety-critical applications such as healthcare, the utility of these models is governed by their ability to generate outputs that are factually accurate and complete. In this work, we present dialog-enabled resolving agents (DERA). DERA is a paradigm made possible by the increased conversational abilities of LLMs, namely GPT-4. It provides a simple, interpretable forum for models to communicate feedback and iteratively improve output. We frame our dialog as a discussion between two agent types - a Researcher, who processes information and identifies crucial problem components, and a Decider, who has the autonomy to integrate the Researcher's information and makes judgments on the final output. We test DERA against three clinically-focused tasks. For medical conversation summarization and care plan generation, DERA shows significant improvement over the base GPT-4 performance in both human expert preference evaluations and quantitative metrics. In a new finding, we also show that GPT-4's performance (70\%) on an open-ended version of the MedQA question-answering (QA) dataset (Jin et al. 2021, USMLE) is well above the passing level (60\%), with DERA showing similar performance. We release the open-ended MEDQA dataset at https://github.com/curai/curai-research/tree/main/DERA.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Nair, Varun and Schumacher, Elliot and Tso, Geoffrey and Kannan, Anitha},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17071 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\UE43TH9K\\Nair 等 - 2023 - DERA Enhancing Large Language Model Completions w.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\JM2J8C85\\2303.html:text/html},
}

@misc{park_generative_2023,
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	shorttitle = {Generative {Agents}},
	url = {http://arxiv.org/abs/2304.03442},
	doi = {10.48550/arXiv.2304.03442},
	abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03442 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\U7GDL86R\\Park 等 - 2023 - Generative Agents Interactive Simulacra of Human .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\V3YG4JVZ\\2304.html:text/html},
}

@misc{wu_promptchainer_2022,
	title = {{PromptChainer}: {Chaining} {Large} {Language} {Model} {Prompts} through {Visual} {Programming}},
	shorttitle = {{PromptChainer}},
	url = {http://arxiv.org/abs/2203.06566},
	doi = {10.48550/arXiv.2203.06566},
	abstract = {While LLMs can effectively help prototype single ML functionalities, many real-world applications involve complex tasks that cannot be easily handled via a single run of an LLM. Recent work has found that chaining multiple LLM runs together (with the output of one step being the input to the next) can help users accomplish these more complex tasks, and in a way that is perceived to be more transparent and controllable. However, it remains unknown what users need when authoring their own LLM chains -- a key step for lowering the barriers for non-AI-experts to prototype AI-infused applications. In this work, we explore the LLM chain authoring process. We conclude from pilot studies find that chaining requires careful scaffolding for transforming intermediate node outputs, as well as debugging the chain at multiple granularities; to help with these needs, we designed PromptChainer, an interactive interface for visually programming chains. Through case studies with four people, we show that PromptChainer supports building prototypes for a range of applications, and conclude with open questions on scaling chains to complex tasks, and supporting low-fi chain prototyping.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Wu, Tongshuang and Jiang, Ellen and Donsbach, Aaron and Gray, Jeff and Molina, Alejandra and Terry, Michael and Cai, Carrie J.},
	month = mar,
	year = {2022},
	note = {arXiv:2203.06566 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\4D858Q3A\\Wu 等 - 2022 - PromptChainer Chaining Large Language Model Promp.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\R7LTHYVQ\\2203.html:text/html},
}

@misc{chen_teaching_2023,
	title = {Teaching {Large} {Language} {Models} to {Self}-{Debug}},
	url = {http://arxiv.org/abs/2304.05128},
	doi = {10.48550/arXiv.2304.05128},
	abstract = {Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3\%, and improves the prediction accuracy on problems of the hardest label by 9\%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12\%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Chen, Xinyun and Lin, Maxwell and Schärli, Nathanael and Zhou, Denny},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05128 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\UTIRE4TV\\Chen 等 - 2023 - Teaching Large Language Models to Self-Debug.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\IIT4MJEX\\2304.html:text/html},
}

@misc{boiko_emergent_2023,
	title = {Emergent autonomous scientific research capabilities of large language models},
	url = {http://arxiv.org/abs/2304.05332},
	doi = {10.48550/arXiv.2304.05332},
	abstract = {Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Boiko, Daniil A. and MacKnight, Robert and Gomes, Gabe},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05332 [physics]},
	keywords = {Computer Science - Computation and Language, Physics - Chemical Physics},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\6GGBLLNB\\Boiko 等 - 2023 - Emergent autonomous scientific research capabiliti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\3CMAA9MF\\2304.html:text/html},
}

@misc{zhong_agieval_2023,
	title = {{AGIEval}: {A} {Human}-{Centric} {Benchmark} for {Evaluating} {Foundation} {Models}},
	shorttitle = {{AGIEval}},
	url = {http://arxiv.org/abs/2304.06364},
	doi = {10.48550/arXiv.2304.06364},
	abstract = {Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95\% accuracy rate on the SAT Math test and a 92.5\% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal these models' strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models' performance in real-world scenarios. The data, code, and all model outputs are released in https://github.com/microsoft/AGIEval.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06364 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\ZGA55AH8\\Zhong 等 - 2023 - AGIEval A Human-Centric Benchmark for Evaluating .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\DMCPFA8G\\2304.html:text/html},
}

@misc{zhang_one_2023,
	title = {One {Small} {Step} for {Generative} {AI}, {One} {Giant} {Leap} for {AGI}: {A} {Complete} {Survey} on {ChatGPT} in {AIGC} {Era}},
	shorttitle = {One {Small} {Step} for {Generative} {AI}, {One} {Giant} {Leap} for {AGI}},
	url = {http://arxiv.org/abs/2304.06488},
	doi = {10.48550/arXiv.2304.06488},
	abstract = {OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Zhang, Chaoning and Zhang, Chenshuang and Li, Chenghao and Qiao, Yu and Zheng, Sheng and Dam, Sumit Kumar and Zhang, Mengchun and Kim, Jung Uk and Kim, Seong Tae and Choi, Jinwoo and Park, Gyeong-Moon and Bae, Sung-Ho and Lee, Lik-Hang and Hui, Pan and Kweon, In So and Hong, Choong Seon},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06488 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\4HE78PNM\\Zhang 等 - 2023 - One Small Step for Generative AI, One Giant Leap f.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\UAAC9PTP\\2304.html:text/html},
}

@misc{zhang_understanding_2023,
	title = {Understanding {Causality} with {Large} {Language} {Models}: {Feasibility} and {Opportunities}},
	shorttitle = {Understanding {Causality} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2304.05524},
	doi = {10.48550/arXiv.2304.05524},
	abstract = {We assess the ability of large language models (LLMs) to answer causal questions by analyzing their strengths and weaknesses against three types of causal question. We believe that current LLMs can answer causal questions with existing causal knowledge as combined domain experts. However, they are not yet able to provide satisfactory answers for discovering new knowledge or for high-stakes decision-making tasks with high precision. We discuss possible future directions and opportunities, such as enabling explicit and implicit causal modules as well as deep causal-aware LLMs. These will not only enable LLMs to answer many different types of causal questions for greater impact but also enable LLMs to be more trustworthy and efficient in general.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Zhang, Cheng and Bauer, Stefan and Bennett, Paul and Gao, Jiangfeng and Gong, Wenbo and Hilmkil, Agrin and Jennings, Joel and Ma, Chao and Minka, Tom and Pawlowski, Nick and Vaughan, James},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05524 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\W8P3XL38\\Zhang 等 - 2023 - Understanding Causality with Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\ZLBVBDBQ\\2304.html:text/html},
}

@misc{pitis_boosted_2023,
	title = {Boosted {Prompt} {Ensembles} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2304.05970},
	doi = {10.48550/arXiv.2304.05970},
	abstract = {Methods such as chain-of-thought prompting and self-consistency have pushed the frontier of language model reasoning performance with no additional training. To further improve performance, we propose a prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble''. The few shot examples for each prompt are chosen in a stepwise fashion to be ``hard'' examples on which the previous step's ensemble is uncertain. We show that this outperforms single-prompt output-space ensembles and bagged prompt-space ensembles on the GSM8k and AQuA datasets, among others. We propose both train-time and test-time versions of boosted prompting that use different levels of available annotation and conduct a detailed empirical study of our algorithm.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Pitis, Silviu and Zhang, Michael R. and Wang, Andrew and Ba, Jimmy},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05970 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\D7ZARZNT\\Pitis 等 - 2023 - Boosted Prompt Ensembles for Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\DBS72UNI\\2304.html:text/html},
}

@misc{liu_global_2023,
	title = {Global {Prompt} {Cell}: {A} {Portable} {Control} {Module} for {Effective} {Prompt}},
	shorttitle = {Global {Prompt} {Cell}},
	url = {http://arxiv.org/abs/2304.05642},
	doi = {10.48550/arXiv.2304.05642},
	abstract = {As a novel approach to tuning pre-trained models, prompt tuning involves freezing the parameters in downstream tasks while inserting trainable embeddings into inputs in the first layer.However,previous methods have mainly focused on the initialization of prompt embeddings. The question of how to train and utilize prompt embeddings in a reasonable way has become aa limiting factor in the effectiveness of prompt tuning. To address this issue, we introduce the Global Prompt Cell (GPC), a portable control module for prompt tuning that selectively preserves prompt information across all encoder layers. Our experimental results demonstrate a 5.8\% improvement on SuperGLUE datasets compared to vanilla prompt tuning.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Liu, Chi and Wang, Haochun and Xi, Nuwa and Zhao, Sendong and Qin, Bing},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05642 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\QI7TA2YA\\Liu 等 - 2023 - Global Prompt Cell A Portable Control Module for .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\6FGYMKA8\\2304.html:text/html},
}

@misc{sun_generative_2023,
	title = {Generative {Knowledge} {Selection} for {Knowledge}-{Grounded} {Dialogues}},
	url = {http://arxiv.org/abs/2304.04836},
	doi = {10.48550/arXiv.2304.04836},
	abstract = {Knowledge selection is the key in knowledge-grounded dialogues (KGD), which aims to select an appropriate knowledge snippet to be used in the utterance based on dialogue history. Previous studies mainly employ the classification approach to classify each candidate snippet as "relevant" or "irrelevant" independently. However, such approaches neglect the interactions between snippets, leading to difficulties in inferring the meaning of snippets. Moreover, they lack modeling of the discourse structure of dialogue-knowledge interactions. We propose a simple yet effective generative approach for knowledge selection, called GenKS. GenKS learns to select snippets by generating their identifiers with a sequence-to-sequence model. GenKS therefore captures intra-knowledge interaction inherently through attention mechanisms. Meanwhile, we devise a hyperlink mechanism to model the dialogue-knowledge interactions explicitly. We conduct experiments on three benchmark datasets, and verify GenKS achieves the best results on both knowledge selection and response generation.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Sun, Weiwei and Ren, Pengjie and Ren, Zhaochun},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04836 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\ZTNMGMDS\\Sun 等 - 2023 - Generative Knowledge Selection for Knowledge-Groun.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\6JJZADGL\\2304.html:text/html},
}

@misc{lei_conditional_2023,
	title = {Conditional {Adapters}: {Parameter}-efficient {Transfer} {Learning} with {Fast} {Inference}},
	shorttitle = {Conditional {Adapters}},
	url = {http://arxiv.org/abs/2304.04947},
	doi = {10.48550/arXiv.2304.04947},
	abstract = {We propose Conditional Adapter (CoDA), a parameter-efficient transfer learning method that also improves inference efficiency. CoDA generalizes beyond standard adapter approaches to enable a new way of balancing speed and accuracy using conditional computation. Starting with an existing dense pretrained model, CoDA adds sparse activation together with a small number of new parameters and a light-weight training phase. Our experiments demonstrate that the CoDA approach provides an unexpectedly efficient way to transfer knowledge. Across a variety of language, vision, and speech tasks, CoDA achieves a 2x to 8x inference speed-up compared to the state-of-the-art Adapter approach with moderate to no accuracy loss and the same parameter efficiency.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Lei, Tao and Bai, Junwen and Brahma, Siddhartha and Ainslie, Joshua and Lee, Kenton and Zhou, Yanqi and Du, Nan and Zhao, Vincent Y. and Wu, Yuexin and Li, Bo and Zhang, Yu and Chang, Ming-Wei},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04947 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\MT4PXEHV\\Lei 等 - 2023 - Conditional Adapters Parameter-efficient Transfer.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\VEADBWFU\\2304.html:text/html},
}

@misc{deshpande_toxicity_2023,
	title = {Toxicity in {ChatGPT}: {Analyzing} {Persona}-assigned {Language} {Models}},
	shorttitle = {Toxicity in {ChatGPT}},
	url = {http://arxiv.org/abs/2304.05335},
	doi = {10.48550/arXiv.2304.05335},
	abstract = {Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and harmful to an unsuspecting user. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Deshpande, Ameet and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05335 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\RM8SDZ47\\Deshpande 等 - 2023 - Toxicity in ChatGPT Analyzing Persona-assigned La.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\VVPALUBP\\2304.html:text/html},
}

@misc{li_multi-step_2023,
	title = {Multi-step {Jailbreaking} {Privacy} {Attacks} on {ChatGPT}},
	url = {http://arxiv.org/abs/2304.05197},
	doi = {10.48550/arXiv.2304.05197},
	abstract = {With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given good prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's model APIs and New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause more severe privacy threats ever than before. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Li, Haoran and Guo, Dadi and Fan, Wei and Xu, Mingshi and Song, Yangqiu},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05197 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\AGE6QUZG\\Li 等 - 2023 - Multi-step Jailbreaking Privacy Attacks on ChatGPT.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\S9DPDZBH\\2304.html:text/html},
}

@misc{zhang_extractive_2023,
	title = {Extractive {Summarization} via {ChatGPT} for {Faithful} {Summary} {Generation}},
	url = {http://arxiv.org/abs/2304.04193},
	doi = {10.48550/arXiv.2304.04193},
	abstract = {Extractive summarization is a crucial task in natural language processing that aims to condense long documents into shorter versions by directly extracting sentences. The recent introduction of ChatGPT has attracted significant interest in the NLP community due to its remarkable performance on a wide range of downstream tasks. However, concerns regarding factuality and faithfulness have hindered its practical applications for summarization systems. This paper first presents a thorough evaluation of ChatGPT's performance on extractive summarization and compares it with traditional fine-tuning methods on various benchmark datasets. Our experimental analysis reveals that ChatGPT's extractive summarization performance is still inferior to existing supervised systems in terms of ROUGE scores. In addition, we explore the effectiveness of in-context learning and chain-of-thought reasoning for enhancing its performance. Furthermore, we find that applying an extract-then-generate pipeline with ChatGPT yields significant performance improvements over abstractive baselines in terms of summary faithfulness. These observations highlight potential directions for enhancing ChatGPT's capabilities for faithful text summarization tasks using two-stage approaches.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Zhang, Haopeng and Liu, Xiao and Zhang, Jiawei},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04193 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\Y8DFWDEK\\Zhang 等 - 2023 - Extractive Summarization via ChatGPT for Faithful .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\SEKMQ58T\\2304.html:text/html},
}

@misc{prystawski_why_2023,
	title = {Why think step-by-step? {Reasoning} emerges from the locality of experience},
	shorttitle = {Why think step-by-step?},
	url = {http://arxiv.org/abs/2304.03843},
	doi = {10.48550/arXiv.2304.03843},
	abstract = {Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare language models' ability to match conditional probabilities both with and without intermediate reasoning steps, finding that intermediate steps help only when the training data is locally structured with respect to dependencies between variables. Furthermore, intermediate variables need to be relevant to the relationship between observed information and target inferences. Our results illustrate how the statistical structure of training data drives the effectiveness of reasoning step by step.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Prystawski, Ben and Goodman, Noah D.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03843 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, todo},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\33ZBFGC7\\Prystawski 和 Goodman - 2023 - Why think step-by-step Reasoning emerges from the.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\MA7FAG6B\\2304.html:text/html},
}

@misc{ge_openagi_2023,
	title = {{OpenAGI}: {When} {LLM} {Meets} {Domain} {Experts}},
	shorttitle = {{OpenAGI}},
	url = {http://arxiv.org/abs/2304.04370},
	doi = {10.48550/arXiv.2304.04370},
	abstract = {Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language queries, serving as input to the LLM. The LLM subsequently selects, synthesizes, and executes models provided by OpenAGI to address the task. Furthermore, we propose a Reinforcement Learning from Task Feedback (RLTF) mechanism, which uses the task-solving result as feedback to improve the LLM's task-solving ability. Thus, the LLM is responsible for synthesizing various external models for solving complex tasks, while RLTF provides feedback to improve its task-solving ability, enabling a feedback loop for self-improving AI. We believe that the paradigm of LLMs operating various expert models for complex task-solving is a promising approach towards AGI. To facilitate the community's long-term improvement and evaluation of AGI's ability, we open-source the code, benchmark, and evaluation methods of the OpenAGI project at https://github.com/agiresearch/OpenAGI.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Ge, Yingqiang and Hua, Wenyue and Ji, Jianchao and Tan, Juntao and Xu, Shuyuan and Zhang, Yongfeng},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04370 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\YMLDA79M\\Ge 等 - 2023 - OpenAGI When LLM Meets Domain Experts.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\5T3DANBW\\2304.html:text/html},
}

@misc{wang_is_2023,
	title = {Is {ChatGPT} a {Good} {Sentiment} {Analyzer}? {A} {Preliminary} {Study}},
	shorttitle = {Is {ChatGPT} a {Good} {Sentiment} {Analyzer}?},
	url = {http://arxiv.org/abs/2304.04339},
	doi = {10.48550/arXiv.2304.04339},
	abstract = {Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly curious about whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of opinions, sentiments, and emotions contained in the text. Specifically, we evaluate it in four settings, including standard evaluation, polarity shift evaluation, open-domain evaluation, and sentiment inference evaluation. The above evaluation involves 18 benchmark datasets and 5 representative sentiment analysis tasks, and we compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on end-task. Moreover, we also conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Wang, Zengzhi and Xie, Qiming and Ding, Zixiang and Feng, Yi and Xia, Rui},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04339 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, dview},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\TJMP5CUT\\Wang 等 - 2023 - Is ChatGPT a Good Sentiment Analyzer A Preliminar.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\UUTNUG6N\\2304.html:text/html},
}

@misc{liu_evaluating_2023,
	title = {Evaluating the {Logical} {Reasoning} {Ability} of {ChatGPT} and {GPT}-4},
	url = {http://arxiv.org/abs/2304.03439},
	doi = {10.48550/arXiv.2304.03439},
	abstract = {Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. GPT-4 shows even higher performance on our manual tests. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known datasets like LogiQA and ReClor. However, the performance drops significantly when handling newly released and out-of-distribution datasets. Logical reasoning remains challenging for ChatGPT and GPT-4, especially on out-of-distribution and natural language inference datasets.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Liu, Hanmeng and Ning, Ruoxi and Teng, Zhiyang and Liu, Jian and Zhou, Qiji and Zhang, Yue},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03439 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\XHIQX2FT\\Liu 等 - 2023 - Evaluating the Logical Reasoning Ability of ChatGP.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\T2BLK7UT\\2304.html:text/html},
}

@misc{yang_evaluations_2023,
	title = {On the {Evaluations} of {ChatGPT} and {Emotion}-enhanced {Prompting} for {Mental} {Health} {Analysis}},
	url = {http://arxiv.org/abs/2304.03347},
	doi = {10.48550/arXiv.2304.03347},
	abstract = {Automated mental health analysis shows great potential for enhancing the efficiency and accessibility of mental health care, whereas the recent dominant methods utilized pre-trained language models (PLMs) as the backbone and incorporated emotional information. The latest large language models (LLMs), such as ChatGPT, exhibit dramatic capabilities on diverse natural language processing tasks. However, existing studies on ChatGPT's zero-shot performance for mental health analysis have limitations in inadequate evaluation, utilization of emotional information, and explainability of methods. In this work, we comprehensively evaluate the mental health analysis and emotional reasoning ability of ChatGPT on 11 datasets across 5 tasks, including binary and multi-class mental health condition detection, cause/factor detection of mental health conditions, emotion recognition in conversations, and causal emotion entailment. We empirically analyze the impact of different prompting strategies with emotional cues on ChatGPT's mental health analysis ability and explainability. Experimental results show that ChatGPT outperforms traditional neural network methods but still has a significant gap with advanced task-specific methods. The qualitative analysis shows its potential in explainability compared with advanced black-box methods but also limitations on robustness and inaccurate reasoning. Prompt engineering with emotional cues is found to be effective in improving its performance on mental health analysis but requires the proper way of emotion infusion.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Yang, Kailai and Ji, Shaoxiong and Zhang, Tianlin and Xie, Qianqian and Ananiadou, Sophia},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03347 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\593FBZE9\\Yang 等 - 2023 - On the Evaluations of ChatGPT and Emotion-enhanced.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\X8DVNCJP\\2304.html:text/html},
}

@misc{zhou_revisiting_2023,
	title = {Revisiting {Automated} {Prompting}: {Are} {We} {Actually} {Doing} {Better}?},
	shorttitle = {Revisiting {Automated} {Prompting}},
	url = {http://arxiv.org/abs/2304.03609},
	doi = {10.48550/arXiv.2304.03609},
	abstract = {Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed, with some progress achieved. In particular, subsequent work demonstrates automation can outperform fine-tuning in certain K-shot learning scenarios. In this paper, we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings. We find that automated prompting does not consistently outperform simple manual prompts. Our work suggests that, in addition to fine-tuning, manual prompts should be used as a baseline in this line of research.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Zhou, Yulin and Zhao, Yiren and Shumailov, Ilia and Mullins, Robert and Gal, Yarin},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03609 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\GKJELEY2\\Zhou 等 - 2023 - Revisiting Automated Prompting Are We Actually Do.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\EWVZG7AM\\2304.html:text/html},
}

@misc{mai_llm_2023,
	title = {{LLM} as {A} {Robotic} {Brain}: {Unifying} {Egocentric} {Memory} and {Control}},
	shorttitle = {{LLM} as {A} {Robotic} {Brain}},
	url = {http://arxiv.org/abs/2304.09349},
	doi = {10.48550/arXiv.2304.09349},
	abstract = {Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The active exploration tasks require the robot to extensively explore an unknown environment within a limited number of actions. Meanwhile, the embodied question answering tasks necessitate that the robot answers questions based on observations acquired during prior explorations.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Mai, Jinjie and Chen, Jun and Li, Bing and Qian, Guocheng and Elhoseiny, Mohamed and Ghanem, Bernard},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09349 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Robotics, dview},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\I4LBT5ZH\\Mai 等 - 2023 - LLM as A Robotic Brain Unifying Egocentric Memory.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\6WUTZIIG\\2304.html:text/html},
}

@misc{patel_pretrained_2023,
	title = {Pretrained {Language} {Models} as {Visual} {Planners} for {Human} {Assistance}},
	url = {http://arxiv.org/abs/2304.09179},
	doi = {10.48550/arXiv.2304.09179},
	abstract = {To make progress towards multi-modal AI assistants which can guide users to achieve complex multi-step goals, we propose the task of Visual Planning for Assistance (VPA). Given a goal briefly described in natural language, e.g., "make a shelf", and a video of the user's progress so far, the aim of VPA is to obtain a plan, i.e., a sequence of actions such as "sand shelf", "paint shelf", etc., to achieve the goal. This requires assessing the user's progress from the untrimmed video, and relating it to the requirements of underlying goal, i.e., relevance of actions and ordering dependencies amongst them. Consequently, this requires handling long video history, and arbitrarily complex action dependencies. To address these challenges, we decompose VPA into video action segmentation and forecasting. We formulate the forecasting step as a multi-modal sequence modeling problem and present Visual Language Model based Planner (VLaMP), which leverages pre-trained LMs as the sequence model. We demonstrate that VLaMP performs significantly better than baselines w.r.t all metrics that evaluate the generated plan. Moreover, through extensive ablations, we also isolate the value of language pre-training, visual observations, and goal information on the performance. We will release our data, model, and code to enable future research on visual planning for assistance.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Patel, Dhruvesh and Eghbalzadeh, Hamid and Kamra, Nitin and Iuzzolino, Michael Louis and Jain, Unnat and Desai, Ruta},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09179 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\HJW4RI7G\\Patel 等 - 2023 - Pretrained Language Models as Visual Planners for .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\9N733Q4Q\\2304.html:text/html},
}

@misc{ge_chain_2023,
	title = {Chain of {Thought} {Prompt} {Tuning} in {Vision} {Language} {Models}},
	url = {http://arxiv.org/abs/2304.07919},
	doi = {10.48550/arXiv.2304.07919},
	abstract = {Language-Image Pre-training has demonstrated promising results on zero-shot and few-shot downstream tasks by prompting visual models with natural language prompts. However, most recent studies only use a single prompt for tuning, neglecting the inherent step-to-step cognitive reasoning process that humans conduct in complex task settings, for example, when processing images from unfamiliar domains. Chain of Thought is a simple and effective approximation to human reasoning process and has been proven useful for natural language processing (NLP) tasks. Based on this cognitive intuition, we believe that conducting effective reasoning is also an important problem in visual tasks, and a chain of thought could be a solution to this problem. In this work, we propose a novel chain of thought prompt tuning for vision-language modeling. Extensive experiments show that our method not only generalizes better in image classification tasks, has greater transferability beyond a single dataset, and has stronger domain generalization performance, but also performs much better in imagetext retrieval and visual question answering, which require more reasoning capabilities. We are the first to successfully adapt chain-of-thought prompting that combines visual and textual embeddings. We will release our codes},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Ge, Jiaxin and Luo, Hongyin and Qian, Siyuan and Gan, Yulu and Fu, Jie and Zhan, Shanghang},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07919 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\PQXQXK72\\Ge 等 - 2023 - Chain of Thought Prompt Tuning in Vision Language .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\R53B843M\\2304.html:text/html},
}

@misc{li_api-bank_2023,
	title = {{API}-{Bank}: {A} {Benchmark} for {Tool}-{Augmented} {LLMs}},
	shorttitle = {{API}-{Bank}},
	url = {http://arxiv.org/abs/2304.08244},
	doi = {10.48550/arXiv.2304.08244},
	abstract = {Recent research has shown that Large Language Models (LLMs) can utilize external tools to improve their contextual processing abilities, moving away from the pure language modeling paradigm and paving the way for Artificial General Intelligence. Despite this, there has been a lack of systematic evaluation to demonstrate the efficacy of LLMs using tools to respond to human instructions. This paper presents API-Bank, the first benchmark tailored for Tool-Augmented LLMs. API-Bank includes 53 commonly used API tools, a complete Tool-Augmented LLM workflow, and 264 annotated dialogues that encompass a total of 568 API calls. These resources have been designed to thoroughly evaluate LLMs' ability to plan step-by-step API calls, retrieve relevant APIs, and correctly execute API calls to meet human needs. The experimental results show that GPT-3.5 emerges the ability to use the tools relative to GPT3, while GPT-4 has stronger planning performance. Nevertheless, there remains considerable scope for further improvement when compared to human performance. Additionally, detailed error analysis and case studies demonstrate the feasibility of Tool-Augmented LLMs for daily use, as well as the primary challenges that future research needs to address.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Li, Minghao and Song, Feifan and Yu, Bowen and Yu, Haiyang and Li, Zhoujun and Huang, Fei and Li, Yongbin},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08244 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\J8HTXKH2\\Li 等 - 2023 - API-Bank A Benchmark for Tool-Augmented LLMs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\GZY38LQN\\2304.html:text/html},
}

@misc{dong_raft_2023,
	title = {{RAFT}: {Reward} {rAnked} {FineTuning} for {Generative} {Foundation} {Model} {Alignment}},
	shorttitle = {{RAFT}},
	url = {http://arxiv.org/abs/2304.06767},
	doi = {10.48550/arXiv.2304.06767},
	abstract = {Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models more effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently assembles a streaming dataset. This dataset serves as the basis for aligning the generative model and can be employed under both offline and online settings. Notably, the sample generation process within RAFT is gradient-free, rendering it compatible with black-box generators. Through extensive experiments, we demonstrate that our proposed algorithm exhibits strong performance in the context of both large language models and diffusion models.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06767 [cs, stat]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\6JCTWN4N\\Dong 等 - 2023 - RAFT Reward rAnked FineTuning for Generative Foun.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\2YIIMXEI\\2304.html:text/html},
}

@misc{zhu_multimodal_2023,
	title = {Multimodal {C4}: {An} {Open}, {Billion}-scale {Corpus} of {Images} {Interleaved} {With} {Text}},
	shorttitle = {Multimodal {C4}},
	url = {http://arxiv.org/abs/2304.06939},
	doi = {10.48550/arXiv.2304.06939},
	abstract = {In-context vision and language models like Flamingo support arbitrarily interleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., "What do image A and image B have in common?" To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available. We release Multimodal C4 (mmc4), an augmentation of the popular text-only c4 corpus with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features, a process that we show outperforms alternatives. mmc4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (90\%) of images are topically relevant, and that linear assignment frequently selects individual sentences specifically well-aligned with each image (78\%). After filtering NSFW images, ads, etc., the corpus contains 103M documents containing 585M images interleaved with 43B English tokens.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Zhu, Wanrong and Hessel, Jack and Awadalla, Anas and Gadre, Samir Yitzhak and Dodge, Jesse and Fang, Alex and Yu, Youngjae and Schmidt, Ludwig and Wang, William Yang and Choi, Yejin},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06939 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\RCJ9WW2E\\Zhu 等 - 2023 - Multimodal C4 An Open, Billion-scale Corpus of Im.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\ASCJHWNV\\2304.html:text/html},
}

@misc{liu_visual_2023,
	title = {Visual {Instruction} {Tuning}},
	url = {http://arxiv.org/abs/2304.08485},
	abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08485 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\HW7P284H\\2304.html:text/html;Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\XNRPRVLQ\\Liu 等 - 2023 - Visual Instruction Tuning.pdf:application/pdf},
}

@misc{qin_tool_2023,
	title = {Tool {Learning} with {Foundation} {Models}},
	url = {http://arxiv.org/abs/2304.08354},
	doi = {10.48550/arXiv.2304.08354},
	abstract = {Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool learning framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate the generalization in tool learning. Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 17 representative tools and show the potential of current foundation models in skillfully utilizing tools. Finally, we discuss several open problems that require further investigation for tool learning. Overall, we hope this paper could inspire future research in integrating tools with foundation models.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Qin, Yujia and Hu, Shengding and Lin, Yankai and Chen, Weize and Ding, Ning and Cui, Ganqu and Zeng, Zheni and Huang, Yufei and Xiao, Chaojun and Han, Chi and Fung, Yi Ren and Su, Yusheng and Wang, Huadong and Qian, Cheng and Tian, Runchu and Zhu, Kunlun and Liang, Shihao and Shen, Xingyu and Xu, Bokai and Zhang, Zhen and Ye, Yining and Li, Bowen and Tang, Ziwei and Yi, Jing and Zhu, Yuzhang and Dai, Zhenning and Yan, Lan and Cong, Xin and Lu, Yaxi and Zhao, Weilin and Huang, Yuxiang and Yan, Junxi and Han, Xu and Sun, Xian and Li, Dahai and Phang, Jason and Yang, Cheng and Wu, Tongshuang and Ji, Heng and Liu, Zhiyuan and Sun, Maosong},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08354 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, todo},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\WH4PM7NL\\Qin 等 - 2023 - Tool Learning with Foundation Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\BJZRJHDP\\2304.html:text/html},
}

@misc{kopf_openassistant_2023,
	title = {{OpenAssistant} {Conversations} -- {Democratizing} {Large} {Language} {Model} {Alignment}},
	url = {http://arxiv.org/abs/2304.07327},
	doi = {10.48550/arXiv.2304.07327},
	abstract = {Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. To demonstrate the OpenAssistant Conversations dataset's effectiveness, we present OpenAssistant, the first fully open-source large-scale instruction-tuned model to be trained on human data. A preference study revealed that OpenAssistant replies are comparably preferred to GPT-3.5-turbo (ChatGPT) with a relative winrate of 48.3\% vs. 51.7\% respectively. We release our code and data under fully permissive licenses.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Köpf, Andreas and Kilcher, Yannic and von Rütte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Richárd and ES, Shahul and Suri, Sameer and Glushkov, David and Dantuluri, Arnav and Maguire, Andrew and Schuhmann, Christoph and Nguyen, Huu and Mattick, Alexander},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07327 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\4LS7CL4N\\Köpf 等 - 2023 - OpenAssistant Conversations -- Democratizing Large.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\TVEDVMSZ\\2304.html:text/html},
}

@misc{zheng_progressive-hint_2023,
	title = {Progressive-{Hint} {Prompting} {Improves} {Reasoning} in {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2304.09797v1},
	abstract = {The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted an extensive and comprehensive evaluation to demonstrate the effectiveness of the proposed method. Our experimental results on six benchmarks show that combining CoT and self-consistency with PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2\% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17\% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (91.9\%), GSM8K (95.5\%) and AQuA (79.9\%).},
	language = {en},
	urldate = {2023-04-21},
	journal = {arXiv.org},
	author = {Zheng, Chuanyang and Liu, Zhengying and Xie, Enze and Li, Zhenguo and Li, Yu},
	month = apr,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\Administrator\\Zotero\\storage\\BC9PTAJ8\\Zheng 等 - 2023 - Progressive-Hint Prompting Improves Reasoning in L.pdf:application/pdf},
}

@misc{lu_chameleon_2023,
	title = {Chameleon: {Plug}-and-{Play} {Compositional} {Reasoning} with {Large} {Language} {Models}},
	shorttitle = {Chameleon},
	url = {http://arxiv.org/abs/2304.09842},
	doi = {10.48550/arXiv.2304.09842},
	abstract = {Large language models (LLMs) have achieved remarkable progress in various natural language processing tasks with emergent abilities. However, they face inherent limitations, such as an inability to access up-to-date information, utilize external tools, or perform precise mathematical reasoning. In this paper, we introduce Chameleon, a plug-and-play compositional reasoning framework that augments LLMs to help address these challenges. Chameleon synthesizes programs to compose various tools, including LLM models, off-the-shelf vision models, web search engines, Python functions, and rule-based modules tailored to user interests. Built on top of an LLM as a natural language planner, Chameleon infers the appropriate sequence of tools to compose and execute in order to generate a final response. We showcase the adaptability and effectiveness of Chameleon on two tasks: ScienceQA and TabMWP. Notably, Chameleon with GPT-4 achieves an 86.54\% accuracy on ScienceQA, significantly improving upon the best published few-shot model by 11.37\%; using GPT-4 as the underlying LLM, Chameleon achieves a 17.8\% increase over the state-of-the-art model, leading to a 98.78\% overall accuracy on TabMWP. Further studies suggest that using GPT-4 as a planner exhibits more consistent and rational tool selection and is able to infer potential constraints given the instructions, compared to other LLMs like ChatGPT.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09842 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Administrator\\Zotero\\storage\\E7H3DF4Y\\Lu 等 - 2023 - Chameleon Plug-and-Play Compositional Reasoning w.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Administrator\\Zotero\\storage\\YMDVN5KB\\2304.html:text/html},
}
@
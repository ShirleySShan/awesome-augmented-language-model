## 2023-4-23
- Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu et al.,**Self-Refine: Iterative Refinement with Self-Feedback**,2023, [[Paper]](http://arxiv.org/abs/2303.17651)
 
- Jojic, Ana and Wang, Zhen and Jojic, Nebojsa,**GPT is becoming a Turing machine: Here are some ways to program it**,2023, [[Paper]](http://arxiv.org/abs/2303.14310)
 
- Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang,**GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment**,2023, [[Paper]](http://arxiv.org/abs/2303.16634)
 
- Bian, Ning and Han, Xianpei and Sun, Le and Lin, Hongyu and Lu, Yaojie and He, Ben,**ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models**,2023, [[Paper]](http://arxiv.org/abs/2303.16421)
 
- Scheurer, Jérémy and Campos, Jon Ander and Korbak, Tomasz and Chan, Jun Shern and Chen, Angelica and Cho, Kyunghyun and Perez, Ethan,**Training Language Models with Language Feedback at Scale**,2023, [[Paper]](http://arxiv.org/abs/2303.16755)
 
- He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, A.-Long and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu,**AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators**,2023, [[Paper]](http://arxiv.org/abs/2303.16854)
 
- Jourdan, Léane and Boudin, Florian and Dufour, Richard and Hernandez, Nicolas,**Text revision in Scientific Writing Assistance: An Overview**,2023, [[Paper]](http://arxiv.org/abs/2303.16726)
 
- Chen, Zichen and Singh, Ambuj K. and Sra, Misha,**LMExplainer: a Knowledge-Enhanced Explainer for Language Models**,2023, [[Paper]](http://arxiv.org/abs/2303.16537)
 
- Orhan, A. Emin,**Recognition, recall, and retention of few-shot memories in large language models**,2023, [[Paper]](http://arxiv.org/abs/2303.17557)
 
- West, Colin G.,**Advances in apparent conceptual physics reasoning in ChatGPT-4**,2023, [[Paper]](http://arxiv.org/abs/2303.17012)
 
- Kim, Geunwoo and Baldi, Pierre and McAleer, Stephen,**Language Models can Solve Computer Tasks**,2023, [[Paper]](http://arxiv.org/abs/2303.17491)
 
- Wu, Shijie and Irsoy, Ozan and Lu, Steven et al.,**BloombergGPT: A Large Language Model for Finance**,2023, [[Paper]](http://arxiv.org/abs/2303.17564)
 
- Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting,**HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace**,2023, [[Paper]](http://arxiv.org/abs/2303.17580)
 
- Liang, Yaobo and Wu, Chenfei and Song, Ting et al.,**TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs**,2023, [[Paper]](http://arxiv.org/abs/2303.16434)
 
- Madaan, Aman and Yazdanbakhsh, Amir,**Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango**,2022, [[Paper]](http://arxiv.org/abs/2209.07686)
 
- Tandon, Niket and Madaan, Aman and Clark, Peter and Yang, Yiming,**Learning to Repair: Repairing model output errors after deployment using a dynamic memory of feedback**,2022, [[Paper]](http://arxiv.org/abs/2112.09737)
 
- Yu, Fei and Zhang, Hongbo and Wang, Benyou,**Nature Language Reasoning, A Survey**,2023, [[Paper]](http://arxiv.org/abs/2303.14725)
 
- Jia, Zhiwei and Liu, Fangchen and Thumuluri, Vineet and Chen, Linghao and Huang, Zhiao and Su, Hao,**Chain-of-Thought Predictive Control**,2023, [[Paper]](http://arxiv.org/abs/2304.00776)
 
- Chen, Jiuhai and Chen, Lichang and Huang, Heng and Zhou, Tianyi,**When do you need Chain-of-Thought Prompting for ChatGPT?**,2023, [[Paper]](http://arxiv.org/abs/2304.03262)
 
- Francis, Jonathan and Kitamura, Nariaki and Labelle, Felix and Lu, Xiaopeng and Navarro, Ingrid and Oh, Jean,**Core Challenges in Embodied Vision-Language Planning**,2023, [[Paper]](http://arxiv.org/abs/2304.02738)
 
- Khan, Muhammad Arshad and Kenney, Max and Painter, Jack and Kamale, Disha and Batista-Navarro, Riza and Ghalamzan-E, Amir,**Natural Language Robot Programming: NLP integrated with autonomous robotic grasping**,2023, [[Paper]](http://arxiv.org/abs/2304.02993)
 
- Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng,**Instruction Tuning with GPT-4**,2023, [[Paper]](http://arxiv.org/abs/2304.03277)
 
- Noever, David and Noever, Samantha Elizabeth Miller,**The Multimodal And Modular Ai Chef: Complex Recipe Generation From Imagery**,2023, [[Paper]](http://arxiv.org/abs/2304.02016)
 
- Gao, Mingqi and Ruan, Jie and Sun, Renliang and Yin, Xunjian and Yang, Shiping and Wan, Xiaojun,**Human-like Summarization Evaluation with ChatGPT**,2023, [[Paper]](http://arxiv.org/abs/2304.02554)
 
- Liu, Yiheng and Han, Tianle and Ma, Siyuan et al.,**Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models**,2023, [[Paper]](http://arxiv.org/abs/2304.01852)
 
- Hu, Zhiqiang and Lan, Yihuai and Wang, Lei and Xu, Wanyu and Lim, Ee-Peng and Lee, Roy Ka-Wei and Bing, Lidong and Poria, Soujanya,**LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models**,2023, [[Paper]](http://arxiv.org/abs/2304.01933)
 
- Xiong, Honglin and Wang, Sheng and Zhu, Yitao and Zhao, Zihao and Liu, Yuxiao and Wang, Qian and Shen, Dinggang,**DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task**,2023, [[Paper]](http://arxiv.org/abs/2304.01097)
 
- Lin, Baihan and Bouneffouf, Djallel and Cecchi, Guillermo and Varshney, Kush R.,**Towards Healthy AI: Large Language Models Need Therapists Too**,2023, [[Paper]](http://arxiv.org/abs/2304.00416)
 
- Puchert, Patrik and Poonam, Poonam and van Onzenoodt, Christian and Ropinski, Timo,**LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models**,2023, [[Paper]](http://arxiv.org/abs/2304.00457)
 
- Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian,**Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data**,2023, [[Paper]](http://arxiv.org/abs/2304.01196)
 
- Zhao, Wayne Xin and Zhou, Kun and Li, Junyi et al.,**A Survey of Large Language Models**,2023, [[Paper]](http://arxiv.org/abs/2303.18223)
 
- Nair, Varun and Schumacher, Elliot and Tso, Geoffrey and Kannan, Anitha,**DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents**,2023, [[Paper]](http://arxiv.org/abs/2303.17071)
 
- Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.,**Generative Agents: Interactive Simulacra of Human Behavior**,2023, [[Paper]](http://arxiv.org/abs/2304.03442)
 
- Wu, Tongshuang and Jiang, Ellen and Donsbach, Aaron and Gray, Jeff and Molina, Alejandra and Terry, Michael and Cai, Carrie J.,**PromptChainer: Chaining Large Language Model Prompts through Visual Programming**,2022, [[Paper]](http://arxiv.org/abs/2203.06566)
 
- Chen, Xinyun and Lin, Maxwell and Schärli, Nathanael and Zhou, Denny,**Teaching Large Language Models to Self-Debug**,2023, [[Paper]](http://arxiv.org/abs/2304.05128)
 
- Boiko, Daniil A. and MacKnight, Robert and Gomes, Gabe,**Emergent autonomous scientific research capabilities of large language models**,2023, [[Paper]](http://arxiv.org/abs/2304.05332)
 
- Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan,**AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models**,2023, [[Paper]](http://arxiv.org/abs/2304.06364)
 
- Zhang, Chaoning and Zhang, Chenshuang and Li, Chenghao et al.,**One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era**,2023, [[Paper]](http://arxiv.org/abs/2304.06488)
 
- Zhang, Cheng and Bauer, Stefan and Bennett, Paul and Gao, Jiangfeng and Gong, Wenbo and Hilmkil, Agrin and Jennings, Joel and Ma, Chao and Minka, Tom and Pawlowski, Nick and Vaughan, James,**Understanding Causality with Large Language Models: Feasibility and Opportunities**,2023, [[Paper]](http://arxiv.org/abs/2304.05524)
 
- Pitis, Silviu and Zhang, Michael R. and Wang, Andrew and Ba, Jimmy,**Boosted Prompt Ensembles for Large Language Models**,2023, [[Paper]](http://arxiv.org/abs/2304.05970)
 
- Liu, Chi and Wang, Haochun and Xi, Nuwa and Zhao, Sendong and Qin, Bing,**Global Prompt Cell: A Portable Control Module for Effective Prompt**,2023, [[Paper]](http://arxiv.org/abs/2304.05642)
 
- Sun, Weiwei and Ren, Pengjie and Ren, Zhaochun,**Generative Knowledge Selection for Knowledge-Grounded Dialogues**,2023, [[Paper]](http://arxiv.org/abs/2304.04836)
 
- Lei, Tao and Bai, Junwen and Brahma, Siddhartha and Ainslie, Joshua and Lee, Kenton and Zhou, Yanqi and Du, Nan et al.,**Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference**,2023, [[Paper]](http://arxiv.org/abs/2304.04947)
 
- Deshpande, Ameet and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik,**Toxicity in ChatGPT: Analyzing Persona-assigned Language Models**,2023, [[Paper]](http://arxiv.org/abs/2304.05335)
 
- Li, Haoran and Guo, Dadi and Fan, Wei and Xu, Mingshi and Song, Yangqiu,**Multi-step Jailbreaking Privacy Attacks on ChatGPT**,2023, [[Paper]](http://arxiv.org/abs/2304.05197)
 
- Zhang, Haopeng and Liu, Xiao and Zhang, Jiawei,**Extractive Summarization via ChatGPT for Faithful Summary Generation**,2023, [[Paper]](http://arxiv.org/abs/2304.04193)
 
- Prystawski, Ben and Goodman, Noah D.,**Why think step-by-step? Reasoning emerges from the locality of experience**,2023, [[Paper]](http://arxiv.org/abs/2304.03843)
 
- Ge, Yingqiang and Hua, Wenyue and Ji, Jianchao and Tan, Juntao and Xu, Shuyuan and Zhang, Yongfeng,**OpenAGI: When LLM Meets Domain Experts**,2023, [[Paper]](http://arxiv.org/abs/2304.04370)
 
- Wang, Zengzhi and Xie, Qiming and Ding, Zixiang and Feng, Yi and Xia, Rui,**Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study**,2023, [[Paper]](http://arxiv.org/abs/2304.04339)
 
- Liu, Hanmeng and Ning, Ruoxi and Teng, Zhiyang and Liu, Jian and Zhou, Qiji and Zhang, Yue,**Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4**,2023, [[Paper]](http://arxiv.org/abs/2304.03439)
 
- Yang, Kailai and Ji, Shaoxiong and Zhang, Tianlin and Xie, Qianqian and Ananiadou, Sophia,**On the Evaluations of ChatGPT and Emotion-enhanced Prompting for Mental Health Analysis**,2023, [[Paper]](http://arxiv.org/abs/2304.03347)
 
- Zhou, Yulin and Zhao, Yiren and Shumailov, Ilia and Mullins, Robert and Gal, Yarin,**Revisiting Automated Prompting: Are We Actually Doing Better?**,2023, [[Paper]](http://arxiv.org/abs/2304.03609)
 
- Mai, Jinjie and Chen, Jun and Li, Bing and Qian, Guocheng and Elhoseiny, Mohamed and Ghanem, Bernard,**LLM as A Robotic Brain: Unifying Egocentric Memory and Control**,2023, [[Paper]](http://arxiv.org/abs/2304.09349)
 
- Patel, Dhruvesh and Eghbalzadeh, Hamid and Kamra, Nitin and Iuzzolino, Michael Louis and Jain, Unnat and Desai, Ruta,**Pretrained Language Models as Visual Planners for Human Assistance**,2023, [[Paper]](http://arxiv.org/abs/2304.09179)
 
- Ge, Jiaxin and Luo, Hongyin and Qian, Siyuan and Gan, Yulu and Fu, Jie and Zhan, Shanghang,**Chain of Thought Prompt Tuning in Vision Language Models**,2023, [[Paper]](http://arxiv.org/abs/2304.07919)
 
- Li, Minghao and Song, Feifan and Yu, Bowen and Yu, Haiyang and Li, Zhoujun and Huang, Fei and Li, Yongbin,**API-Bank: A Benchmark for Tool-Augmented LLMs**,2023, [[Paper]](http://arxiv.org/abs/2304.08244)
 
- Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong,**RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment**,2023, [[Paper]](http://arxiv.org/abs/2304.06767)
 
- Zhu, Wanrong and Hessel, Jack and Awadalla, Anas and Gadre, Samir Yitzhak and Dodge, Jesse and Fang, Alex and Yu, Youngjae and Schmidt, Ludwig and Wang, William Yang and Choi, Yejin,**Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text**,2023, [[Paper]](http://arxiv.org/abs/2304.06939)
 
- Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae,**Visual Instruction Tuning**,2023, [[Paper]](http://arxiv.org/abs/2304.08485)
 
- Qin, Yujia and Hu, Shengding and Lin, Yankai and Chen, Weize and Ding, Ning and Cui, Ganqu et al.,**Tool Learning with Foundation Models**,2023, [[Paper]](http://arxiv.org/abs/2304.08354)
 
- Köpf, Andreas and Kilcher, Yannic and von Rütte, Dimitri and Anagnostidis, Sotiris et al., Alexander,**OpenAssistant Conversations -- Democratizing Large Language Model Alignment**,2023, [[Paper]](http://arxiv.org/abs/2304.07327)
 
- Zheng, Chuanyang and Liu, Zhengying and Xie, Enze and Li, Zhenguo and Li, Yu,**Progressive-Hint Prompting Improves Reasoning in Large Language Models**,2023, [[Paper]](https://arxiv.org/abs/2304.09797v1)
 
- Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng,**Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models**,2023, [[Paper]](http://arxiv.org/abs/2304.09842)